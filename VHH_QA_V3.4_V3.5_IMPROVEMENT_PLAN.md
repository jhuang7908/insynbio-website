# VHH QA v3.4/v3.5 æ”¹è¿›è®¡åˆ’

**æ—¥æœŸ**: 2025å¹´12æœˆ10æ—¥  
**å½“å‰ç‰ˆæœ¬**: v3.3.0  
**ç›®æ ‡ç‰ˆæœ¬**: v3.4.0, v3.5.0

---

## ä¸€ã€é—®é¢˜åˆ†æ

### âŒ é—®é¢˜1: final_scoreæƒé‡ä½“ç³»è¿‡äºå¯å‘å¼

**å½“å‰å®ç°**:
```python
final_score = combined - 0.20 * structural_risk - hallmark_penalty
```

**é—®é¢˜**:
1. `structural_risk`çš„å®šä¹‰ä¸å¤Ÿä¸¥æ ¼ï¼ˆscore 0~1çš„æ¥æºæœªå®šä¹‰ï¼‰
2. `0.20`çš„æƒé‡å¤ªéšæ„ï¼ˆä¸æ˜¯åŸºäºåˆ†å¸ƒæ ¡å‡†ï¼‰
3. `hallmark_penalty = 0.15`ä¹Ÿéœ€è¦ç”Ÿç‰©ç‰©ç†æ¥æº

**å½±å“**: 
- æƒé‡ç¼ºä¹ç§‘å­¦ä¾æ®
- æ— æ³•é€‚åº”ä¸åŒæ•°æ®åˆ†å¸ƒ
- éš¾ä»¥è§£é‡Šå’Œè°ƒè¯•

---

### âŒ é—®é¢˜2: ranking sanityè§„åˆ™ä¾èµ–é˜ˆå€¼ï¼Œè€Œéå­¦ä¹ å‹æ¨¡å‹

**å½“å‰å®ç°**:
```python
if fr_gap >= 0.10 and comb_gap <= 0.03:
    errors.append("Ranking sanity violated...")
```

**é—®é¢˜**:
1. ä»ç„¶æ˜¯heuristicï¼ˆç»éªŒé˜ˆå€¼ï¼‰
2. æ²¡æœ‰è€ƒè™‘pairwise consistency
3. æ²¡æœ‰ä½¿ç”¨isotonic regressionæ ¡å‡†

**å½±å“**:
- é˜ˆå€¼å¯èƒ½ä¸é€‚åˆæ‰€æœ‰åœºæ™¯
- æ— æ³•å­¦ä¹ æ•°æ®ä¸­çš„æ¨¡å¼
- ç¼ºä¹ç›¸å¯¹æ’åºç¨³å®šæ€§åˆ¤æ–­

---

### âŒ é—®é¢˜3: ç»“æ„é£é™©æ²¡æœ‰åˆ†å±‚

**å½“å‰å®ç°**:
- `structural_risk`æ˜¯ä¸€ç»´é‡ï¼ˆ0~1ï¼‰
- åªæ•è·äº†grafting impactçš„è¿‘ä¼¼

**ç¼ºå¤±çš„ç»´åº¦**:
1. **FR2 hydrophilic patchå®Œæ•´æ€§**ï¼ˆ37/44/45/47ï¼‰
2. **Graftingåœ¨interfaceä¸Šçš„èƒ½é‡å˜åŒ–**ï¼ˆÎ”Î”G proxyï¼‰
3. **CDR3 anchor residues**ï¼ˆ95/96/101/102ï¼‰æ˜¯å¦åŒ¹é…æ¨¡æ¿å‡ ä½•ç»“æ„

**å½±å“**:
- å¯¹VHHæ¥è¯´ï¼ŒCDR3 anchoræ˜¯ç”Ÿæ­»çº¿
- æ— æ³•åŒºåˆ†ä¸åŒæ¥æºçš„ç»“æ„é£é™©
- å¯èƒ½å¯¼è‡´å…³é”®é£é™©è¢«å¿½ç•¥

---

## äºŒã€v3.4æ”¹è¿›è®¡åˆ’

### ğŸ¯ ç›®æ ‡ï¼šåŸºäºæ•°æ®åˆ†å¸ƒæ ¡å‡†çš„æƒé‡ä½“ç³» + åˆ†å±‚ç»“æ„é£é™©

---

### æ”¹è¿›1: å¼•å…¥ç»éªŒæ•°æ®åˆ†å¸ƒæ ¡å‡†ï¼ˆDistribution Calibrationï¼‰

#### 1.1 æ•°æ®æ”¶é›†æ¨¡å—

**æ–‡ä»¶**: `core/vhh_qa_data_calibration.py`

**åŠŸèƒ½**:
```python
class VHHDataCalibration:
    """
    åŸºäºå†å²VHHæ•°æ®åº“ç»Ÿè®¡ï¼Œæ ¡å‡†QAé˜ˆå€¼å’Œæƒé‡
    """
    
    def __init__(self, calibration_db_path: str):
        """
        åŠ è½½æ ¡å‡†æ•°æ®åº“
        
        æ•°æ®åº“ç»“æ„ï¼š
        {
            "successful_cases": [
                {
                    "structural_risk": float,
                    "has_hallmark": bool,
                    "cdr3_anchor_match": bool,
                    "final_outcome": "success"
                },
                ...
            ],
            "failed_cases": [
                {
                    "structural_risk": float,
                    "has_hallmark": bool,
                    "cdr3_anchor_match": bool,
                    "final_outcome": "failed",
                    "failure_reason": str
                },
                ...
            ]
        }
        """
        self.db = self._load_calibration_db(calibration_db_path)
        self._compute_distributions()
    
    def _compute_distributions(self):
        """è®¡ç®—æˆåŠŸ/å¤±è´¥æ¡ˆä¾‹çš„åˆ†å¸ƒç»Ÿè®¡"""
        # æˆåŠŸæ¡ˆä¾‹çš„structural_riskåˆ†å¸ƒ
        success_risks = [c["structural_risk"] for c in self.db["successful_cases"]]
        self.success_risk_median = np.median(success_risks)
        self.success_risk_p75 = np.percentile(success_risks, 75)
        
        # å¤±è´¥æ¡ˆä¾‹çš„structural_riskåˆ†å¸ƒ
        failed_risks = [c["structural_risk"] for c in self.db["failed_cases"]]
        self.failed_risk_median = np.median(failed_risks)
        self.failed_risk_p25 = np.percentile(failed_risks, 25)
        
        # è®¡ç®—æ ¡å‡†æƒé‡
        self._calibrate_weights()
    
    def _calibrate_weights(self):
        """
        åŸºäºåˆ†å¸ƒå·®å¼‚æ ¡å‡†æƒé‡
        
        åŸç†ï¼š
        - å¦‚æœæˆåŠŸæ¡ˆä¾‹çš„median risk = 0.2ï¼Œå¤±è´¥æ¡ˆä¾‹çš„median risk = 0.6
        - åˆ™riskå·®å¼‚ = 0.4ï¼Œè¿™æ˜¯"æœ‰æ„ä¹‰çš„å·®å¼‚"
        - æƒé‡åº”è¯¥ä½¿å¾—è¿™ä¸ªå·®å¼‚èƒ½å¤Ÿæ˜¾è‘—å½±å“final_score
        """
        risk_diff = self.failed_risk_median - self.success_risk_median
        
        # ç›®æ ‡ï¼šä½¿å¾—riskå·®å¼‚èƒ½å¤Ÿäº§ç”Ÿè‡³å°‘0.1çš„final_scoreå·®å¼‚
        # å³ï¼šweight * risk_diff >= 0.1
        self.structural_risk_weight = max(0.1, 0.1 / risk_diff) if risk_diff > 0 else 0.2
        
        # Hallmark penaltyæ ¡å‡†
        success_with_hallmark = sum(1 for c in self.db["successful_cases"] 
                                   if c.get("has_hallmark", True))
        failed_without_hallmark = sum(1 for c in self.db["failed_cases"] 
                                     if not c.get("has_hallmark", True))
        
        total_success = len(self.db["successful_cases"])
        total_failed = len(self.db["failed_cases"])
        
        if total_success > 0 and total_failed > 0:
            hallmark_success_rate = success_with_hallmark / total_success
            hallmark_failure_rate = failed_without_hallmark / total_failed
            
            # Hallmarkç¼ºå¤±çš„å¤±è´¥ç‡å·®å¼‚
            hallmark_impact = hallmark_failure_rate - (1 - hallmark_success_rate)
            self.hallmark_penalty = max(0.05, min(0.25, hallmark_impact))
        else:
            self.hallmark_penalty = 0.15  # é»˜è®¤å€¼
    
    def get_calibrated_weights(self) -> Dict[str, float]:
        """è¿”å›æ ¡å‡†åçš„æƒé‡"""
        return {
            "structural_risk_weight": self.structural_risk_weight,
            "hallmark_penalty": self.hallmark_penalty,
            "success_risk_median": self.success_risk_median,
            "failed_risk_median": self.failed_risk_median,
            "calibration_source": "VHH_historical_database"
        }
```

#### 1.2 æ›´æ–°final_scoreè®¡ç®—

**æ–‡ä»¶**: `core/vhh_qa_validation_v3_4.py`

```python
def compute_final_score_v3_4(
    candidate: Dict[str, Any],
    calibration: Optional[VHHDataCalibration] = None
) -> float:
    """
    v3.4: ä½¿ç”¨æ ¡å‡†æƒé‡çš„final_scoreè®¡ç®—
    
    Args:
        candidate: å€™é€‰æ¨¡æ¿å­—å…¸
        calibration: æ•°æ®æ ¡å‡†å™¨ï¼ˆå¦‚æœæä¾›ï¼Œä½¿ç”¨æ ¡å‡†æƒé‡ï¼›å¦åˆ™ä½¿ç”¨é»˜è®¤å€¼ï¼‰
    """
    scores = candidate.get("alignment_scores", {}) or candidate.get("scores", {})
    base = scores.get("combined_score", 0) or scores.get("combined", 0)
    
    # è·å–structural_riskï¼ˆéœ€è¦ä»åˆ†å±‚é£é™©è®¡ç®—ï¼‰
    structural_risk = _compute_layered_structural_risk(candidate)
    
    # è·å–æ ¡å‡†æƒé‡
    if calibration:
        weights = calibration.get_calibrated_weights()
        structural_risk_weight = weights["structural_risk_weight"]
        hallmark_penalty = weights["hallmark_penalty"]
    else:
        # é»˜è®¤å€¼ï¼ˆå‘åå…¼å®¹ï¼‰
        structural_risk_weight = 0.20
        hallmark_penalty = 0.15
    
    # Hallmark penalty
    flags = candidate.get("flags", {}) or {}
    template = candidate.get("template", {})
    if isinstance(template, dict):
        template_flags = template.get("flags", {}) or {}
        flags = {**flags, **template_flags}
    
    actual_hallmark_penalty = 0.0
    if not flags.get("has_vhh_hallmark", True):
        actual_hallmark_penalty = hallmark_penalty
    elif flags.get("reduced_hallmark", False):
        actual_hallmark_penalty = hallmark_penalty * 0.33  # å‡å°‘çš„penalty
    
    final = base - structural_risk_weight * structural_risk - actual_hallmark_penalty
    
    # æ›´æ–°candidateçš„scores
    if "scores" not in candidate:
        candidate["scores"] = {}
    candidate["scores"]["final"] = final
    candidate["scores"]["structural_risk"] = structural_risk
    candidate["scores"]["structural_risk_weight"] = structural_risk_weight
    candidate["scores"]["hallmark_penalty"] = actual_hallmark_penalty
    
    return final
```

---

### æ”¹è¿›2: åˆ†å±‚ç»“æ„é£é™©ï¼ˆLayered Structural Riskï¼‰

#### 2.1 å®šä¹‰åˆ†å±‚é£é™©ç»“æ„

**æ–‡ä»¶**: `core/vhh_qa_structural_risk_layered.py`

```python
from typing import Dict, List, Tuple, Optional
from dataclasses import dataclass

@dataclass
class StructuralRiskComponents:
    """ç»“æ„é£é™©çš„åˆ†å±‚ç»„ä»¶"""
    fr2_hydrophilic_patch_risk: float  # 0~1, FR2 hydrophilic patchå®Œæ•´æ€§
    grafting_interface_risk: float      # 0~1, Graftingåœ¨interfaceä¸Šçš„èƒ½é‡å˜åŒ–
    cdr3_anchor_risk: float            # 0~1, CDR3 anchor residuesåŒ¹é…åº¦
    
    @property
    def total_risk(self) -> float:
        """
        åŠ æƒç»„åˆæ€»é£é™©
        
        æƒé‡ï¼š
        - FR2 risk: 0.3ï¼ˆé‡è¦ä½†å¯å®¹å¿ï¼‰
        - Grafting risk: 0.3ï¼ˆé‡è¦ä½†å¯å®¹å¿ï¼‰
        - CDR3 anchor risk: 0.4ï¼ˆç”Ÿæ­»çº¿ï¼Œæƒé‡æœ€é«˜ï¼‰
        """
        return (
            0.3 * self.fr2_hydrophilic_patch_risk +
            0.3 * self.grafting_interface_risk +
            0.4 * self.cdr3_anchor_risk
        )
    
    def to_dict(self) -> Dict[str, float]:
        """è½¬æ¢ä¸ºå­—å…¸"""
        return {
            "fr2_hydrophilic_patch_risk": self.fr2_hydrophilic_patch_risk,
            "grafting_interface_risk": self.grafting_interface_risk,
            "cdr3_anchor_risk": self.cdr3_anchor_risk,
            "total_risk": self.total_risk
        }


def compute_layered_structural_risk(
    orig_regions: Dict[str, str],
    hum_regions: Dict[str, str],
    template_info: Optional[Dict[str, Any]] = None
) -> StructuralRiskComponents:
    """
    è®¡ç®—åˆ†å±‚ç»“æ„é£é™©
    
    Args:
        orig_regions: åŸå§‹åºåˆ—åŒºåŸŸ
        hum_regions: äººæºåŒ–åºåˆ—åŒºåŸŸ
        template_info: æ¨¡æ¿ä¿¡æ¯ï¼ˆç”¨äºanchoråŒ¹é…ï¼‰
    
    Returns:
        StructuralRiskComponents
    """
    # 1. FR2 hydrophilic patché£é™©
    fr2_risk = _compute_fr2_hydrophilic_patch_risk(orig_regions, hum_regions)
    
    # 2. Grafting interfaceé£é™©
    grafting_risk = _compute_grafting_interface_risk(orig_regions, hum_regions)
    
    # 3. CDR3 anchoré£é™©ï¼ˆå…³é”®ï¼‰
    anchor_risk = _compute_cdr3_anchor_risk(orig_regions, hum_regions, template_info)
    
    return StructuralRiskComponents(
        fr2_hydrophilic_patch_risk=fr2_risk,
        grafting_interface_risk=grafting_risk,
        cdr3_anchor_risk=anchor_risk
    )


def _compute_fr2_hydrophilic_patch_risk(
    orig_regions: Dict[str, str],
    hum_regions: Dict[str, str]
) -> float:
    """
    è®¡ç®—FR2 hydrophilic patchå®Œæ•´æ€§é£é™©
    
    VHH hallmarkä½ç½®ï¼š37, 44, 45, 47
    è¿™äº›ä½ç½®å½¢æˆhydrophilic patchï¼Œå¯¹å•åŸŸæŠ˜å è‡³å…³é‡è¦
    """
    VHH_HALLMARK_POSITIONS = [37, 44, 45, 47]
    
    orig_fr2 = orig_regions.get("FR2", "")
    hum_fr2 = hum_regions.get("FR2", "")
    
    if not orig_fr2 or not hum_fr2:
        return 1.0  # ç¼ºå¤±FR2ï¼Œé£é™©æœ€é«˜
    
    # æ£€æŸ¥æ¯ä¸ªhallmarkä½ç½®çš„ä¿ç•™æƒ…å†µ
    preserved_count = 0
    for pos in VHH_HALLMARK_POSITIONS:
        # è½¬æ¢ä¸ºFR2å†…çš„ç´¢å¼•ï¼ˆFR2ä»IMGT 39å¼€å§‹ï¼‰
        fr2_start = 39
        local_idx = pos - fr2_start
        
        if 0 <= local_idx < len(orig_fr2) and 0 <= local_idx < len(hum_fr2):
            orig_aa = orig_fr2[local_idx]
            hum_aa = hum_fr2[local_idx]
            
            # å¦‚æœä¿ç•™æˆ–å˜ä¸ºæ›´hydrophilicçš„æ®‹åŸºï¼Œç®—ä¿ç•™
            if orig_aa == hum_aa:
                preserved_count += 1
            elif _is_hydrophilic_improvement(orig_aa, hum_aa):
                preserved_count += 1
    
    # é£é™© = 1 - ä¿ç•™æ¯”ä¾‹
    risk = 1.0 - (preserved_count / len(VHH_HALLMARK_POSITIONS))
    return max(0.0, min(1.0, risk))


def _compute_grafting_interface_risk(
    orig_regions: Dict[str, str],
    hum_regions: Dict[str, str]
) -> float:
    """
    è®¡ç®—graftingåœ¨interfaceä¸Šçš„èƒ½é‡å˜åŒ–é£é™©ï¼ˆÎ”Î”G proxyï¼‰
    
    ä½¿ç”¨ç°æœ‰çš„qa_grafting_impactï¼Œä½†è½¬æ¢ä¸º0~1çš„é£é™©åˆ†æ•°
    """
    from core.vhh_qa_grafting import qa_grafting_impact
    
    _, _, impact_details = qa_grafting_impact(orig_regions, hum_regions)
    impact_normalized = impact_details.get("impact_score_normalized", 0)
    
    # å½’ä¸€åŒ–åˆ°0~1é£é™©åˆ†æ•°
    # impact_normalizedé€šå¸¸åœ¨0~1ä¹‹é—´ï¼Œä½†å¯èƒ½è¶…è¿‡1
    risk = min(1.0, impact_normalized / 0.4)  # 0.4æ˜¯erroré˜ˆå€¼
    return risk


def _compute_cdr3_anchor_risk(
    orig_regions: Dict[str, str],
    hum_regions: Dict[str, str],
    template_info: Optional[Dict[str, Any]] = None
) -> float:
    """
    è®¡ç®—CDR3 anchor residuesåŒ¹é…åº¦é£é™©ï¼ˆç”Ÿæ­»çº¿ï¼‰
    
    CDR3 anchorä½ç½®ï¼šIMGT 95, 96, 101, 102
    è¿™äº›ä½ç½®åœ¨FR3ä¸­ï¼Œå¯¹CDR3æ„å‹è‡³å…³é‡è¦
    
    å¦‚æœæ¨¡æ¿101/102ä¸ºæŸç§ç±»å‹ï¼Œhumanizedä¹Ÿè¦ä¿æŒä¸€è‡´
    å¦åˆ™structural_risk â‰¥ 0.7 â†’ å¿…é¡»fail
    """
    CDR3_ANCHOR_POSITIONS = [95, 96, 101, 102]  # IMGTä½ç½®
    
    orig_fr3 = orig_regions.get("FR3", "")
    hum_fr3 = hum_regions.get("FR3", "")
    
    if not orig_fr3 or not hum_fr3:
        return 1.0  # ç¼ºå¤±FR3ï¼Œé£é™©æœ€é«˜
    
    # FR3ä»IMGT 66å¼€å§‹
    fr3_start = 66
    
    # æ£€æŸ¥æ¯ä¸ªanchorä½ç½®çš„åŒ¹é…æƒ…å†µ
    mismatches = 0
    critical_mismatches = 0  # 101/102æ˜¯å…³é”®ä½ç½®
    
    for pos in CDR3_ANCHOR_POSITIONS:
        local_idx = pos - fr3_start
        
        if 0 <= local_idx < len(orig_fr3) and 0 <= local_idx < len(hum_fr3):
            orig_aa = orig_fr3[local_idx]
            hum_aa = hum_fr3[local_idx]
            
            if orig_aa != hum_aa:
                mismatches += 1
                # 101/102æ˜¯å…³é”®ä½ç½®
                if pos in [101, 102]:
                    critical_mismatches += 1
    
    # é£é™©è®¡ç®—
    if critical_mismatches > 0:
        # å…³é”®ä½ç½®ä¸åŒ¹é…ï¼Œé£é™©æé«˜
        risk = 0.7 + (critical_mismatches * 0.15)  # è‡³å°‘0.7ï¼Œæ¯ä¸ªå…³é”®mismatch +0.15
    else:
        # éå…³é”®ä½ç½®ä¸åŒ¹é…ï¼Œé£é™©è¾ƒä½
        risk = mismatches * 0.2  # æ¯ä¸ªmismatch +0.2
    
    # å¦‚æœæ¨¡æ¿ä¿¡æ¯å¯ç”¨ï¼Œæ£€æŸ¥æ¨¡æ¿çš„anchorç±»å‹
    if template_info:
        template_fr3 = template_info.get("fr3_sequence", "")
        if template_fr3:
            template_anchor_101 = template_fr3[101 - fr3_start] if 101 - fr3_start < len(template_fr3) else None
            template_anchor_102 = template_fr3[102 - fr3_start] if 102 - fr3_start < len(template_fr3) else None
            
            hum_anchor_101 = hum_fr3[101 - fr3_start] if 101 - fr3_start < len(hum_fr3) else None
            hum_anchor_102 = hum_fr3[102 - fr3_start] if 102 - fr3_start < len(hum_fr3) else None
            
            # å¦‚æœhumanizedçš„anchorä¸æ¨¡æ¿ä¸åŒ¹é…ï¼Œé£é™©æé«˜
            if template_anchor_101 and hum_anchor_101 and template_anchor_101 != hum_anchor_101:
                risk = max(risk, 0.8)
            if template_anchor_102 and hum_anchor_102 and template_anchor_102 != hum_anchor_102:
                risk = max(risk, 0.8)
    
    return max(0.0, min(1.0, risk))
```

#### 2.2 æ›´æ–°QAéªŒè¯é€»è¾‘

**æ–‡ä»¶**: `core/vhh_qa_validation_v3_4.py`

```python
def validate_vhh_humanization_result_v3_4(
    result: Dict[str, Any],
    strict: bool = True,
    calibration: Optional[VHHDataCalibration] = None
) -> Dict[str, Any]:
    """
    VHHäººæºåŒ–ç»“æœQAéªŒè¯ v3.4
    
    v3.4å‡çº§ï¼š
    - åŸºäºæ•°æ®åˆ†å¸ƒæ ¡å‡†çš„æƒé‡ä½“ç³»
    - åˆ†å±‚ç»“æ„é£é™©ï¼ˆFR2/grafting/CDR3 anchorï¼‰
    """
    # ... (å¤ç”¨v3.3çš„åŸºç¡€æ£€æŸ¥)
    
    # === v3.4: è®¡ç®—åˆ†å±‚ç»“æ„é£é™© ===
    orig_regions = seq_analysis.get("original_regions", {}) or {}
    hum_regions = seq_analysis.get("humanized_regions", {}) or {}
    template_info = result.get("best_match", {}).get("template", {})
    
    structural_risk_components = compute_layered_structural_risk(
        orig_regions, hum_regions, template_info
    )
    
    # æ£€æŸ¥CDR3 anchoré£é™©ï¼ˆç”Ÿæ­»çº¿ï¼‰
    if structural_risk_components.cdr3_anchor_risk >= 0.7:
        errors.append(
            f"CDR3 anchor residuesé£é™©è¿‡é«˜ ({structural_risk_components.cdr3_anchor_risk:.2f})ï¼Œ"
            "è¿™æ˜¯VHHæŠ˜å çš„ç”Ÿæ­»çº¿ã€‚æ¨¡æ¿101/102ä½ç½®ä¸humanizedä¸åŒ¹é…ï¼Œ"
            "å¯èƒ½å¯¼è‡´ç»“æ„ä¸ç¨³å®šæˆ–æ— æ³•æŠ˜å ã€‚"
        )
    
    # === v3.4: ä½¿ç”¨æ ¡å‡†æƒé‡è®¡ç®—final_score ===
    candidates = result.get("candidates", [])
    if candidates:
        for cand in candidates:
            # è®¡ç®—åˆ†å±‚ç»“æ„é£é™©
            cand_orig_regions = ...  # ä»candidateè·å–
            cand_hum_regions = ...   # ä»candidateè·å–
            cand_risk_components = compute_layered_structural_risk(
                cand_orig_regions, cand_hum_regions, cand.get("template", {})
            )
            
            # æ›´æ–°candidateçš„structural_risk
            if "scores" not in cand:
                cand["scores"] = {}
            cand["scores"]["structural_risk"] = cand_risk_components.total_risk
            cand["scores"]["structural_risk_components"] = cand_risk_components.to_dict()
            
            # ä½¿ç”¨æ ¡å‡†æƒé‡è®¡ç®—final_score
            compute_final_score_v3_4(cand, calibration)
    
    # ... (å…¶ä½™é€»è¾‘)
```

---

## ä¸‰ã€v3.5æ”¹è¿›è®¡åˆ’

### ğŸ¯ ç›®æ ‡ï¼šç›¸å¯¹æ’åºç¨³å®šæ€§æ¨¡å‹ï¼ˆRanking Stability Modelï¼‰

---

### æ”¹è¿›3: å¼•å…¥ç›¸å¯¹æ’åºç¨³å®šæ€§æ¨¡å‹

#### 3.1 å®šä¹‰æ’åºç¨³å®šæ€§æ¨¡å‹

**æ–‡ä»¶**: `core/vhh_qa_ranking_stability.py`

```python
from typing import Dict, List, Tuple, Optional
from dataclasses import dataclass
import numpy as np
from sklearn.isotonic import IsotonicRegression

@dataclass
class RankingStabilityResult:
    """æ’åºç¨³å®šæ€§åˆ†æç»“æœ"""
    is_stable: bool
    stability_score: float  # 0~1, è¶Šé«˜è¶Šç¨³å®š
    swap_risk: float        # 0~1, å¦‚æœbestå’Œsecondäº’æ¢çš„é£é™©
    consistency_issues: List[str]
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            "is_stable": self.is_stable,
            "stability_score": self.stability_score,
            "swap_risk": self.swap_risk,
            "consistency_issues": self.consistency_issues
        }


def analyze_ranking_stability(
    candidates: List[Dict[str, Any]],
    calibration: Optional[VHHDataCalibration] = None
) -> RankingStabilityResult:
    """
    åˆ†ææ’åºç¨³å®šæ€§
    
    åŸç†ï¼š
    1. è®¡ç®—bestå’Œsecondäº’æ¢åçš„é£é™©å˜åŒ–
    2. å¦‚æœå·®å¼‚ < é˜ˆå€¼ â†’ ranking unstable
    3. ä½¿ç”¨pairwise consistencyåˆ¤æ–­
    4. å¯é€‰ï¼šä½¿ç”¨isotonic regressionæ ¡å‡†score consistency
    """
    if len(candidates) < 2:
        return RankingStabilityResult(
            is_stable=True,
            stability_score=1.0,
            swap_risk=0.0,
            consistency_issues=[]
        )
    
    best = candidates[0]
    second = candidates[1]
    
    # 1. è®¡ç®—äº’æ¢åçš„é£é™©å˜åŒ–
    swap_risk = _compute_swap_risk(best, second, calibration)
    
    # 2. Pairwise consistencyæ£€æŸ¥
    consistency_issues = _check_pairwise_consistency(candidates)
    
    # 3. è®¡ç®—ç¨³å®šæ€§åˆ†æ•°
    stability_score = 1.0 - swap_risk
    if consistency_issues:
        stability_score -= len(consistency_issues) * 0.1
    
    stability_score = max(0.0, min(1.0, stability_score))
    
    # 4. åˆ¤æ–­æ˜¯å¦ç¨³å®š
    is_stable = stability_score >= 0.7 and swap_risk < 0.3
    
    return RankingStabilityResult(
        is_stable=is_stable,
        stability_score=stability_score,
        swap_risk=swap_risk,
        consistency_issues=consistency_issues
    )


def _compute_swap_risk(
    best: Dict[str, Any],
    second: Dict[str, Any],
    calibration: Optional[VHHDataCalibration] = None
) -> float:
    """
    è®¡ç®—bestå’Œsecondäº’æ¢åçš„é£é™©å˜åŒ–
    
    å¦‚æœäº’æ¢åé£é™©æ˜¾è‘—å¢åŠ ï¼Œè¯´æ˜å½“å‰æ’åºæ˜¯åˆç†çš„
    å¦‚æœäº’æ¢åé£é™©å˜åŒ–å¾ˆå°ï¼Œè¯´æ˜æ’åºä¸ç¨³å®š
    """
    # è·å–å½“å‰final_score
    best_final = best.get("scores", {}).get("final", 0)
    second_final = second.get("scores", {}).get("final", 0)
    
    current_gap = best_final - second_final
    
    # æ¨¡æ‹Ÿäº’æ¢ï¼šè®¡ç®—å¦‚æœsecondæˆä¸ºbestçš„é£é™©
    # è¿™é‡Œä½¿ç”¨structural_riskä½œä¸ºé£é™©ä»£ç†
    best_risk = best.get("scores", {}).get("structural_risk", 0)
    second_risk = second.get("scores", {}).get("structural_risk", 0)
    
    # å¦‚æœsecondçš„riskæ˜¾è‘—é«˜äºbestï¼Œè¯´æ˜äº’æ¢é£é™©å¤§ï¼ˆå½“å‰æ’åºåˆç†ï¼‰
    risk_diff = second_risk - best_risk
    
    # å¦‚æœriskå·®å¼‚å°ä½†final_scoreå·®å¼‚ä¹Ÿå°ï¼Œè¯´æ˜æ’åºä¸ç¨³å®š
    if abs(risk_diff) < 0.1 and abs(current_gap) < 0.05:
        swap_risk = 0.5  # ä¸­ç­‰é£é™©
    elif risk_diff > 0.2:
        swap_risk = 0.1  # ä½é£é™©ï¼ˆäº’æ¢æ˜æ˜¾æ›´å·®ï¼‰
    elif risk_diff < -0.1:
        swap_risk = 0.8  # é«˜é£é™©ï¼ˆsecondå®é™…ä¸Šæ›´å¥½ï¼Ÿï¼‰
    else:
        swap_risk = 0.3  # ä¸­ç­‰é£é™©
    
    return swap_risk


def _check_pairwise_consistency(
    candidates: List[Dict[str, Any]]
) -> List[str]:
    """
    æ£€æŸ¥pairwise consistency
    
    å¯¹äºæ¯å¯¹å€™é€‰æ¨¡æ¿ï¼Œæ£€æŸ¥ï¼š
    - å¦‚æœAçš„FR identity > Bï¼Œä½†Açš„final_score < Bï¼Œè¿™æ˜¯ä¸ä¸€è‡´çš„
    - å¦‚æœAçš„structural_risk < Bï¼Œä½†Açš„final_score < Bï¼Œè¿™ä¹Ÿæ˜¯ä¸ä¸€è‡´çš„
    """
    issues = []
    
    for i in range(len(candidates)):
        for j in range(i + 1, len(candidates)):
            a = candidates[i]
            b = candidates[j]
            
            a_scores = a.get("scores", {}) or a.get("alignment_scores", {})
            b_scores = b.get("scores", {}) or b.get("alignment_scores", {})
            
            a_fr = a_scores.get("fr_identity", 0) or a_scores.get("framework_identity", 0)
            b_fr = b_scores.get("fr_identity", 0) or b_scores.get("framework_identity", 0)
            
            a_final = a_scores.get("final", 0)
            b_final = b_scores.get("final", 0)
            
            # æ£€æŸ¥ä¸ä¸€è‡´æ€§
            if a_fr > b_fr + 0.05 and a_final < b_final - 0.02:
                issues.append(
                    f"å€™é€‰æ¨¡æ¿ {a.get('template_id', f'#{i+1}')} çš„FR identity ({a_fr:.2f}) "
                    f"æ˜¾è‘—é«˜äº {b.get('template_id', f'#{j+1}')} ({b_fr:.2f})ï¼Œ"
                    f"ä½†final_scoreæ›´ä½ ({a_final:.3f} vs {b_final:.3f})ï¼Œæ’åºä¸ä¸€è‡´ã€‚"
                )
    
    return issues


def calibrate_score_consistency(
    candidates: List[Dict[str, Any]]
) -> Dict[str, Any]:
    """
    ä½¿ç”¨isotonic regressionæ ¡å‡†score consistency
    
    ç¡®ä¿combined_scoreå’Œfinal_scoreçš„å•è°ƒæ€§
    """
    # æå–combined_scoreå’Œfinal_score
    combined_scores = []
    final_scores = []
    
    for cand in candidates:
        scores = cand.get("scores", {}) or cand.get("alignment_scores", {})
        combined = scores.get("combined_score", 0) or scores.get("combined", 0)
        final = scores.get("final", combined)
        
        combined_scores.append(combined)
        final_scores.append(final)
    
    if len(combined_scores) < 3:
        return {"calibrated": False, "reason": "insufficient_data"}
    
    # ä½¿ç”¨isotonic regressionæ ¡å‡†
    try:
        ir = IsotonicRegression(out_of_bounds='clip')
        calibrated_final = ir.fit_transform(combined_scores, final_scores)
        
        # æ£€æŸ¥æ ¡å‡†åçš„å•è°ƒæ€§
        is_monotonic = all(calibrated_final[i] >= calibrated_final[i+1] 
                          for i in range(len(calibrated_final)-1))
        
        return {
            "calibrated": True,
            "is_monotonic": is_monotonic,
            "calibration_model": ir,
            "calibrated_scores": calibrated_final.tolist()
        }
    except Exception as e:
        return {"calibrated": False, "reason": str(e)}
```

#### 3.2 æ›´æ–°ranking sanityæ£€æŸ¥

**æ–‡ä»¶**: `core/vhh_qa_validation_v3_5.py`

```python
def _qa_ranking_sanity_v3_5(
    candidates: List[Dict[str, Any]],
    errors: List[str],
    warnings: List[Dict[str, str]],
    calibration: Optional[VHHDataCalibration] = None
) -> Dict[str, Any]:
    """
    v3.5å‡çº§ï¼šä½¿ç”¨æ’åºç¨³å®šæ€§æ¨¡å‹çš„ranking sanityæ£€æŸ¥
    """
    from core.vhh_qa_ranking_stability import (
        analyze_ranking_stability,
        calibrate_score_consistency
    )
    
    sanity_details = {
        "ranking_issues": [],
        "score_consistency": {},
        "stability_analysis": {}
    }
    
    if not candidates or len(candidates) < 2:
        return sanity_details
    
    # 1. æ’åºç¨³å®šæ€§åˆ†æ
    stability_result = analyze_ranking_stability(candidates, calibration)
    sanity_details["stability_analysis"] = stability_result.to_dict()
    
    # 2. Score consistencyæ ¡å‡†
    consistency_result = calibrate_score_consistency(candidates)
    sanity_details["score_consistency"] = consistency_result
    
    # 3. æ ¹æ®ç¨³å®šæ€§ç»“æœç”Ÿæˆerrors/warnings
    if not stability_result.is_stable:
        if stability_result.swap_risk >= 0.7:
            errors.append(
                f"æ’åºä¸ç¨³å®šï¼šæœ€ä½³æ¨¡æ¿å’Œæ¬¡ä¼˜æ¨¡æ¿äº’æ¢åé£é™©å˜åŒ–æå° "
                f"(swap_risk={stability_result.swap_risk:.2f})ï¼Œ"
                "å½“å‰æ’åºå¯èƒ½ä¸æ­£ç¡®ã€‚å»ºè®®é‡æ–°è¯„ä¼°æ¨¡æ¿é€‰æ‹©ç­–ç•¥ã€‚"
            )
        else:
            warnings.append(_create_warning(
                "major",
                "ranking",
                f"æ’åºç¨³å®šæ€§è¾ƒä½ (stability_score={stability_result.stability_score:.2f})ï¼Œ"
                f"å»ºè®®äººå·¥å¤æ ¸æ¨¡æ¿é€‰æ‹©ã€‚"
            ))
    
    # 4. Pairwise consistencyé—®é¢˜
    if stability_result.consistency_issues:
        for issue in stability_result.consistency_issues:
            warnings.append(_create_warning(
                "major",
                "ranking",
                issue
            ))
    
    # 5. Score consistencyé—®é¢˜
    if consistency_result.get("calibrated") and not consistency_result.get("is_monotonic"):
        warnings.append(_create_warning(
            "minor",
            "ranking",
            "Scoreä¸€è‡´æ€§æ ¡å‡†åä»å­˜åœ¨éå•è°ƒæ€§ï¼Œå»ºè®®æ£€æŸ¥è¯„åˆ†æ¨¡å‹ã€‚"
        ))
    
    return sanity_details
```

---

## å››ã€å®æ–½æ—¶é—´è¡¨

### v3.4ï¼ˆé¢„è®¡2å‘¨ï¼‰

**Week 1**:
- [ ] åˆ›å»ºæ•°æ®æ ¡å‡†æ¨¡å—
- [ ] å®ç°åˆ†å±‚ç»“æ„é£é™©è®¡ç®—
- [ ] æ›´æ–°final_scoreè®¡ç®—é€»è¾‘
- [ ] å•å…ƒæµ‹è¯•

**Week 2**:
- [ ] é›†æˆåˆ°QAéªŒè¯æµç¨‹
- [ ] æ›´æ–°æµ‹è¯•ç”¨ä¾‹
- [ ] æ–‡æ¡£æ›´æ–°
- [ ] å®Œæ•´æµ‹è¯•å¥—ä»¶éªŒè¯

### v3.5ï¼ˆé¢„è®¡2å‘¨ï¼‰

**Week 1**:
- [ ] åˆ›å»ºæ’åºç¨³å®šæ€§æ¨¡å‹
- [ ] å®ç°pairwise consistencyæ£€æŸ¥
- [ ] é›†æˆisotonic regression
- [ ] å•å…ƒæµ‹è¯•

**Week 2**:
- [ ] æ›´æ–°ranking sanityæ£€æŸ¥
- [ ] æ›´æ–°æµ‹è¯•ç”¨ä¾‹
- [ ] æ–‡æ¡£æ›´æ–°
- [ ] å®Œæ•´æµ‹è¯•å¥—ä»¶éªŒè¯

---

## äº”ã€æ•°æ®éœ€æ±‚

### æ ¡å‡†æ•°æ®åº“

éœ€è¦æ”¶é›†ä»¥ä¸‹æ•°æ®ï¼š

1. **æˆåŠŸæ¡ˆä¾‹**ï¼ˆè‡³å°‘100ä¸ªï¼‰:
   - structural_riskå€¼
   - has_hallmarkæ ‡å¿—
   - cdr3_anchor_matchæ ‡å¿—
   - final_outcome = "success"

2. **å¤±è´¥æ¡ˆä¾‹**ï¼ˆè‡³å°‘50ä¸ªï¼‰:
   - structural_riskå€¼
   - has_hallmarkæ ‡å¿—
   - cdr3_anchor_matchæ ‡å¿—
   - final_outcome = "failed"
   - failure_reason

### æ•°æ®æ¥æº

- å†…éƒ¨VHHäººæºåŒ–é¡¹ç›®å†å²æ•°æ®
- å…¬å¼€VHHç»“æ„æ•°æ®åº“ï¼ˆSAbDabï¼‰
- æ–‡çŒ®æŠ¥é“çš„VHHäººæºåŒ–æ¡ˆä¾‹

---

## å…­ã€é£é™©è¯„ä¼°

### æŠ€æœ¯é£é™©

1. **æ•°æ®ä¸è¶³**: å¦‚æœæ ¡å‡†æ•°æ®åº“æ ·æœ¬é‡ä¸è¶³ï¼Œæ ¡å‡†å¯èƒ½ä¸å‡†ç¡®
   - **ç¼“è§£**: ä½¿ç”¨bootstrapæ–¹æ³•ä¼°è®¡ç½®ä¿¡åŒºé—´

2. **è®¡ç®—å¤æ‚åº¦**: åˆ†å±‚é£é™©è®¡ç®—å’Œæ’åºç¨³å®šæ€§åˆ†æå¯èƒ½å¢åŠ è®¡ç®—æ—¶é—´
   - **ç¼“è§£**: ä½¿ç”¨ç¼“å­˜å’Œå¹¶è¡Œè®¡ç®—

3. **å‘åå…¼å®¹æ€§**: v3.4/v3.5çš„æƒé‡å˜åŒ–å¯èƒ½å½±å“ç°æœ‰ç»“æœ
   - **ç¼“è§£**: æä¾›å‘åå…¼å®¹æ¨¡å¼ï¼Œå…è®¸ä½¿ç”¨é»˜è®¤æƒé‡

### ä¸šåŠ¡é£é™©

1. **é˜ˆå€¼å˜åŒ–**: æ ¡å‡†åçš„é˜ˆå€¼å¯èƒ½å¯¼è‡´æ›´å¤š/æ›´å°‘çš„failures
   - **ç¼“è§£**: é€æ­¥éƒ¨ç½²ï¼ŒA/Bæµ‹è¯•

2. **è§£é‡Šæ€§**: åˆ†å±‚é£é™©å’Œæ’åºç¨³å®šæ€§æ¨¡å‹å¯èƒ½éš¾ä»¥å‘ç”¨æˆ·è§£é‡Š
   - **ç¼“è§£**: æä¾›è¯¦ç»†çš„æŠ¥å‘Šå’Œå¯è§†åŒ–

---

## ä¸ƒã€æ€»ç»“

### v3.4å…³é”®æ”¹è¿›

1. âœ… åŸºäºæ•°æ®åˆ†å¸ƒæ ¡å‡†çš„æƒé‡ä½“ç³»
2. âœ… åˆ†å±‚ç»“æ„é£é™©ï¼ˆFR2/grafting/CDR3 anchorï¼‰
3. âœ… CDR3 anchoré£é™©ä½œä¸ºç”Ÿæ­»çº¿æ£€æŸ¥

### v3.5å…³é”®æ”¹è¿›

1. âœ… ç›¸å¯¹æ’åºç¨³å®šæ€§æ¨¡å‹
2. âœ… Pairwise consistencyæ£€æŸ¥
3. âœ… Isotonic regressionæ ¡å‡†

### é¢„æœŸæ•ˆæœ

- **æ›´ç§‘å­¦çš„æƒé‡**: åŸºäºæ•°æ®è€Œéç»éªŒ
- **æ›´ç²¾ç¡®çš„é£é™©è¯„ä¼°**: åˆ†å±‚è¯†åˆ«ä¸åŒæ¥æºçš„é£é™©
- **æ›´ç¨³å®šçš„æ’åº**: å‡å°‘æ’åºä¸ä¸€è‡´æ€§
- **æ›´å¥½çš„è§£é‡Šæ€§**: æ˜ç¡®çš„é£é™©æ¥æºå’Œæ’åºä¾æ®

---

**æ–‡æ¡£ç‰ˆæœ¬**: 1.0  
**æœ€åæ›´æ–°**: 2025å¹´12æœˆ10æ—¥

















